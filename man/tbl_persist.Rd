% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dplyr_spark.R
\name{tbl_persist}
\alias{tbl_persist}
\title{Persist a Spark Table}
\usage{
tbl_persist(sc, name, storageLevel = "NONE", force = TRUE)
}
\arguments{
\item{sc}{A \code{spark_connection}.}

\item{name}{The table name.}

\item{storageLevel}{The caching storage level for the spark table.}

\item{force}{Force the data to be loaded into memory? This is accomplished
by calling the \code{count} API on the associated Spark DataFrame.}
}
\description{
By default, this function works as persist function in Spark, so data
is not loaded into memory unless you tell a storage level. Depending on
which storage level you choose, operations on cached tables used should
be more or less performant. See
\href{http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence}{Spark programming guide}
for more details.
}

